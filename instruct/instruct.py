from typing import List

import urllib3
from fastapi import FastAPI
from huggingface_hub import InferenceClient
from opensearchpy import OpenSearch
from pydantic import BaseModel, Field
from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    huggingface_model: str = Field(
        default="mistralai/Mistral-7B-Instruct-v0.3", alias="HUGGINGFACE_MODEL"
    )
    opensearch_host: str = Field(default="localhost", alias="OPENSEARCH_HOST")
    opensearch_port: int = Field(default=9200, alias="OPENSEARCH_PORT")
    opensearch_user: str = Field(default="admin", alias="OPENSEARCH_USER")
    opensearch_password: str = Field(
        default="#Admin1234", alias="OPENSEARCH_PASSWORD"
    )
    opensearch_https: bool = Field(default=True, alias="OPENSEARCH_HTTPS")
    opensearch_verify_ssl: bool = Field(default=False, alias="OPENSEARCH_VERIFY_SSL")
    opensearch_index: str = Field(default="sentences", alias="OPENSEARCH_INDEX")


settings = Settings()

huggingface_client = InferenceClient()

opensearch = OpenSearch(
    hosts=[{"host": settings.opensearch_host, "port": settings.opensearch_port}],
    http_auth=(settings.opensearch_user, settings.opensearch_password),
    use_ssl=settings.opensearch_https,
    verify_certs=settings.opensearch_verify_ssl,
)

app = FastAPI(title="Text Generation API")


class Request(BaseModel):
    prompt: str = Field(description="Prompt or question given by the user")
    sentences_used: int = Field(default=10, description="Number of sentences to be used")
    max_tokens: int = Field(default=500, description="Max number of tokens to be generated")
    temperature: float = Field(default=1.0, description="Temperature for the text generation")
    model: str = Field(default=settings.huggingface_model, description="LLM Model used for text generation")


class Response(BaseModel):
    text: str = Field(description="Text generated by the model")
    sentences: List[str] = Field(description="Sentences used for creating the prompt")


template = """
Você é um agente virtual e precisa responder a seguinte dúvida do usuário:

{user_prompt}

Utilizando informações contidas nos seguintes trechos delimitados por backtick
crie um texto que responda a dúvida do usuário.

```
{sentences}
```

Texto:
"""


@app.post("/", tags=["generation"])
def text_generation(request: Request) -> Response:
    # Busca no OpenSearch as melhores N sentenças utilizando os embeddings indexados
    # O número de sentenças pode ser controlado pelo parâmetro 'request.sentences_used'
    search_response = opensearch.search(
        index=settings.opensearch_index,
        body={
            "_source": {"excludes": ["sentence_embedding"]},
            "size": request.sentences_used,
            "query": {
                "neural": {
                    "sentence_embedding": {
                        "query_text": request.prompt,
                        "k": request.sentences_used,
                    }
                }
            },
        },
    )

    sentences = [doc["_source"]["sentence"] for doc in search_response["hits"]["hits"]]

    # Cria prompt utlizando a entrada do usuário e as sentenças retornadas pela busca
    prompt = template.format(user_prompt=request.prompt, sentences="\n".join(sentences))

    # Gera um texto utilizando o modelo desejado
    # 'request.model' seleciona um modelo disponibilizado no HuggingFace
    # 'request.max_tokens' controla o tamanho máximo do texto
    # 'request.temperature' controla a critividade da resposta do modelo
    text = huggingface_client.text_generation(
        model=request.model,
        prompt=prompt,
        max_new_tokens=request.max_tokens,
        temperature=request.temperature,
    )

    return Response(text=text, sentences=sentences)
